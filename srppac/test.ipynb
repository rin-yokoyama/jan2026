{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9b5709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/15 01:53:44 WARN Utils: Your hostname, gpuana02, resolves to a loopback address: 127.0.1.1; using 192.168.1.216 instead (on interface enp4s0)\n",
      "26/01/15 01:53:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "26/01/15 01:53:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/15 01:53:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/01/15 01:53:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "26/01/15 01:53:45 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "26/01/15 01:53:45 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "26/01/15 01:53:45 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "26/01/15 01:53:45 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+----+\n",
      "|hbfNumber| ch|     time|edge|\n",
      "+---------+---+---------+----+\n",
      "| 12261748| 42|427374760|   0|\n",
      "| 12261748| 22|427375129|   0|\n",
      "| 12261748| 23|427375495|   0|\n",
      "| 12261748| 41|427375973|   0|\n",
      "| 12261748| 43|427377582|   0|\n",
      "+---------+---+---------+----+\n",
      "only showing top 5 rows\n",
      "+---------+--------------------+--------------------+----+\n",
      "|hbfNumber|                time|                 tot|size|\n",
      "+---------+--------------------+--------------------+----+\n",
      "| 12261748|         [427411599]|             [83315]|   1|\n",
      "| 12261749|[222935691, 22902...|[83336, 83305, 83...|   4|\n",
      "| 12261750|[224389826, 44441...|[83342, 83321, 83...|   3|\n",
      "| 12261751|[11665250, 231942...|[83378, 83371, 83...|   3|\n",
      "| 12261753|[281113819, 45394...|      [83306, 83308]|   2|\n",
      "+---------+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "+---------+------------+--------------------+--------------------+------+------------------+------------------+------------------+-------------------+-----+-----+-----+------------+--------------------+--------------------+------+------------------+-------------------+------------------+------------------+-----+-----+-----+-------+\n",
      "|hbfNumber|        id_x|            timing_x|            charge_x|size_x|         timing0_x|         charge0_x|         charge1_x|          charge2_x|id0_x|id1_x|id2_x|        id_y|            timing_y|            charge_y|size_y|         timing0_y|          charge0_y|         charge1_y|         charge2_y|id0_y|id1_y|id2_y|runname|\n",
      "+---------+------------+--------------------+--------------------+------+------------------+------------------+------------------+-------------------+-----+-----+-----+------------+--------------------+--------------------+------+------------------+-------------------+------------------+------------------+-----+-----+-----+-------+\n",
      "| 12317872|        [17]|[1.9659447430785804]|[10.557789424663497]|     1|1.9659447430785804|10.557789424663497|              NULL|               NULL|   17| NULL| NULL|    [24, 20]|[0.14419014649683...|[16.0291597966606...|     2|0.1441901464968396|  16.02915979666068| 15.02332950710479|              NULL|   24|   20| NULL|run1007|\n",
      "| 12318405|    [24, 25]|[38.8221816602454...|[-36.635142561979...|     2|38.822181660245406|-36.63514256197959|-38.72991475669551|               NULL|   24|   25| NULL|    [22, 21]|[45.3716738757648...|[-35.015432799918...|     2| 45.37167387576483|-35.015432799918926|-38.20112032101315|              NULL|   22|   21| NULL|run1007|\n",
      "| 12324091|[18, 15, 18]|[17.7267410469212...|[29.1010340047505...|     3|17.726741046921234|29.101034004750545|19.841808532422874|-17.322455185931176|   18|   15|   18|[20, 17, 21]|[17.3148315134021...|[31.4868858036206...|     3| 17.31483151340217|  31.48688580362068| 21.88519353031006|11.391145036148373|   20|   17|   21|run1007|\n",
      "+---------+------------+--------------------+--------------------+------+------------------+------------------+------------------+-------------------+-----+-----+-----+------------+--------------------+--------------------+------+------------------+-------------------+------------------+------------------+-----+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark._jvm.decoders.HRTDCDecoder.registerUDF(spark._jsparkSession)\n",
    "spark._jvm.decoders.HRTDCUnpairedDecoder.registerUDF(spark._jsparkSession)\n",
    "\n",
    "df = spark.read.parquet(\"/home/h487/data/parquet/run1007.parquet\")\n",
    "#df.show(10)\n",
    "\n",
    "# Filter SRPPAC strip data and decode\n",
    "df_src = df.filter(\"femType==7 and femId==615\").select(\"data\").withColumn(\"decoded\",F.expr(\"decode_hrtdc_unpaired_segdata(data)\"))\n",
    "df_src = df_src.select(\"decoded.*\").select(\"hbf.*\",\"data\").select(\"hbfNumber\",\"data\").filter(\"array_size(decoded.data)>0\")\n",
    "df_src = df_src.withColumn(\"ex\",F.explode(\"data\")).select(\"hbfNumber\",\"ex.*\").withColumnRenamed(\"tot\",\"edge\")\n",
    "#df_src.filter(\"hbfNumber=10057285 and ch=41\").show()\n",
    "df_src.show(5)\n",
    "\n",
    "# Filter SRPPAC anode data and decode\n",
    "df_sra = df.filter(\"femType==5 and femId==614\").select(\"data\").withColumn(\"decoded\",F.expr(\"decode_hrtdc_segdata(data)\"))\n",
    "df_sra = df_sra.select(\"decoded.*\").select(\"hbf.*\",\"data\").select(\"hbfNumber\",\"data\").filter(\"array_size(decoded.data)>0\")\n",
    "df_sra = df_sra.withColumn(\"ex\",F.explode(\"data\")).select(\"hbfNumber\",\"ex.*\").filter(\"ch==4\")\n",
    "df_sra = df_sra.groupBy(\"hbfNumber\").agg(F.collect_list(\"time\").alias(\"time\"),F.collect_list(\"tot\").alias(\"tot\")) # select fastest anode hit\n",
    "df_sra = df_sra.withColumn(\"size\", F.size(\"time\"))\n",
    "df_sra.show(5)\n",
    "\n",
    "check = spark.read.parquet(\"/home/h487/data/parquet/run1007_srppac.parquet\")\n",
    "check.show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-oedo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
